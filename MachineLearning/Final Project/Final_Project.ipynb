{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["You can find this ipynb also in github, on"],"metadata":{"id":"CKuNMavIX8ML"}},{"cell_type":"markdown","source":["# Import Libraries"],"metadata":{"id":"328Zl6T8ENi-"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"jSXT_AE0EAra","executionInfo":{"status":"ok","timestamp":1705962466374,"user_tz":-120,"elapsed":6,"user":{"displayName":"Βαγγέλης Κουκόλης","userId":"10082600920396367384"}}},"outputs":[],"source":["# Here we import the libraries we will use\n","import time\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import train_test_split\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.neighbors import KNeighborsRegressor\n","import tensorflow as tf\n","\n","def install_agent_libraries():\n","  !pip install swig\n","  !pip install gymnasium[box2d]\n","  !pip install ray[rllib]\n","  !pip install renderlab\n","#install_agent_libraries()"]},{"cell_type":"code","source":["# Check if GPU is enabled\n","!nvidia-smi -L"],"metadata":{"id":"Y-Uosneaf4s4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705962469625,"user_tz":-120,"elapsed":339,"user":{"displayName":"Βαγγέλης Κουκόλης","userId":"10082600920396367384"}},"outputId":"0901c3ef-ff4c-4112-925d-b38c595a83a0"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["GPU 0: Tesla T4 (UUID: GPU-f46e924d-811e-6149-4d51-80f296d971cd)\n"]}]},{"cell_type":"markdown","source":["# 1. Express the moves (rock, scissors, paper) into (0, 1, 2) respectively"],"metadata":{"id":"iScrPyOHTK8i"}},{"cell_type":"markdown","source":["# 2. Split the dataset into train-test: You can for each class select a ratio as test set (e.g. 20% of rock, 20% of scissors and 20% of paper) and use these to test your model/agent."],"metadata":{"id":"s648E4u0Tp4L"}},{"cell_type":"markdown","source":["# 3. Select Image: Select a radom image frm the 2100 (which corresponds to move 0, or 1, or 2)"],"metadata":{"id":"PrAlwiF1T3Yi"}},{"cell_type":"markdown","source":["# 4. Preprocess Image: Process the image:\n","*  a. wih probability $p_1$ apply Vertical flip\n","*  b. with probability $p_2$ apply Horizontal flip\n","*  c. Add noise with $\\mu = 0$, $\\sigma = 255\\cdot 0.05$. If you normalize the image, adjust standard deviation accordingly.\n","*  d. You can apply any other method Image Processing you want, so that you can make the game harder (if you want).\n"],"metadata":{"id":"aGAh9siwT8ka"}},{"cell_type":"markdown","source":["# 5. Apply: Send the image to your agent"],"metadata":{"id":"ZoMDy2TAUcFj"}},{"cell_type":"markdown","source":["# 6. The agent read the image and selects the optimal action"],"metadata":{"id":"XKmZEhOfUfGq"}},{"cell_type":"markdown","source":["# 7. The goal is maximizing gain. So, you will need to plot the agent's gain (You can store the total gain for every round and plot it at the end of the game)"],"metadata":{"id":"nD6HiKLpVNWc"}},{"cell_type":"markdown","source":["# 8. As end goal, try the accuracy of the agent (or model) on images outside of the dataset, e.g. from the intenret or your own, which you must rescale to the same size. For example, you can try the action it will output for the attached image"],"metadata":{"id":"AM2e3GGbVfWC"}},{"cell_type":"markdown","source":["# 5. Create an agent. Use summart to print the policy model. Describe the created Neural Network."],"metadata":{"id":"yqsl5B9GnP3G"}},{"cell_type":"code","source":["def import_agent():\n","  import gymnasium as gym\n","  import ray\n","  from ray.rllib.algorithms.ppo import PPOConfig\n","#import_agent()"],"metadata":{"id":"x8CkSp8Sm16T","executionInfo":{"status":"ok","timestamp":1705962509452,"user_tz":-120,"elapsed":263,"user":{"displayName":"Βαγγέλης Κουκόλης","userId":"10082600920396367384"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["def agent_definition_01():\n","  ray.shutdown()\n","  ray.init()\n","\n","  config = PPOConfig()\n","  config.use_critic = True # Use Actor-Critic Architecture\n","  config.use_gae = True # Use Generalzed Advantage Updates\n","  config.lambda_ = 0.95 # Lambda Learning parameter\n","  config.user_kl_loss = True # Use KL-Penalty in addition to e-Clipping\n","  config.sgd_minibatch_size = 32 # Batch Size\n","  config.num_sgd_iter = 30 # Epochs\n","  config.shuffle_sequences = True # Whether to shffle episode sequences\n","  config.vf_loss_coeff = 0.5 # Value Function loss Regularization\n","  config.entropy_coeff = 0.001 # Entropy Regularization coefficient for Exploration\n","  config.clip_param = 0.2 # e-Clipping parameter\n","\n","  agent = config.environment(\"LunarLander-v2\").framework(framework='tf').build()\n","  agent.get_policy().model.base_model.summary(expand_nested=True)\n","#agent_definition_01()"],"metadata":{"id":"6cGwqHwAnapY","executionInfo":{"status":"ok","timestamp":1705962542107,"user_tz":-120,"elapsed":6,"user":{"displayName":"Βαγγέλης Κουκόλης","userId":"10082600920396367384"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["# 6. Create a train loop to train the agent"],"metadata":{"id":"XYtaqCGzAblw"}},{"cell_type":"code","source":["def agent_train():\n","  train_iterations = 100\n","  average_rewards_per_iteration = []\n","\n","  for i in range(train_iterations):\n","    log = agent.train()\n","    average_rewards = log['sampler_results']['episode_reward_mean']\n","    average_rewards_per_iteration.append(average_rewards)\n","\n","    print(f'Iteration {i + 1}, Average Rewards: {average_rewards}')\n","\n","\n","  plt.plot(average_rewards_per_iteration)\n","  plt.title('PPO LunarLander-v2')\n","  plt.xlabel('Iterations')\n","  plt.ylabel('Average Rewards')\n","  plt.show()\n","#agent_train()"],"metadata":{"id":"oT4QBNisAgCW","executionInfo":{"status":"ok","timestamp":1705962595975,"user_tz":-120,"elapsed":9,"user":{"displayName":"Βαγγέλης Κουκόλης","userId":"10082600920396367384"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["# 7. Use the library renderlab to \"render\" the gym environmonet in colab, to see how the trained agent plays."],"metadata":{"id":"gPPKfQUCJB1R"}},{"cell_type":"code","source":["#import renderlab as rl"],"metadata":{"id":"wXw37XZUJQEA","executionInfo":{"status":"ok","timestamp":1705962605871,"user_tz":-120,"elapsed":268,"user":{"displayName":"Βαγγέλης Κουκόλης","userId":"10082600920396367384"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["def agent_run():\n","  env = gym.make(\"LunarLander-v2\", render_mode = \"rgb_array\")\n","  env = rl.RenderFrame(env, \"./output\")\n","\n","  observation, info = env.reset()\n","\n","  while True:\n","    action = env.action_space.sample()\n","    observation,reward, terminated, truncated, info = env.step(action)\n","\n","    if terminated or truncated:\n","      break\n","\n","  env.play()\n","#agent_run()"],"metadata":{"id":"spqBDwwpJO0l","executionInfo":{"status":"ok","timestamp":1705962635255,"user_tz":-120,"elapsed":8,"user":{"displayName":"Βαγγέλης Κουκόλης","userId":"10082600920396367384"}}},"execution_count":10,"outputs":[]}]}